TASK_CONFIG = {
    'ifeval': {'dataset_path': 'google/IFEval', 'dataset_name': None, 'split': 'train'},
    'gsm8k' : {'dataset_path': 'openai/gsm8k', 'dataset_name': 'main', 'split': 'test'},
    'hellaswag_gen' : {'dataset_path': 'Rowan/hellaswag', 'dataset_name': None, 'split': 'test'},
    'drop_new' : {'dataset_path': 'EleutherAI/drop', 'dataset_name': None, 'split': 'validation'},
    'truthfulqa_gen': {'dataset_path' : 'truthfulqa/truthful_qa', 'dataset_name': 'generation', 'split': 'validation'},
    'mmlu_generative': {'dataset_path': 'hails/mmlu_no_train', 'dataset_name': None, 'split': 'test'},
    'mmlu_stem_generative': {'dataset_path': 'hails/mmlu_no_train', 
                            'dataset_name': ['abstract_algebra',
                                            'anatomy',
                                            'astronomy',
                                            'college_biology',
                                            'college_chemistry',
                                            'college_computer_science',
                                            'college_mathematics',
                                            'college_physics',
                                            'computer_security',
                                            'conceptual_physics',
                                            'electrical_engineering',
                                            'elementary_mathematics',
                                            'high_school_biology',
                                            'high_school_chemistry',
                                            'high_school_computer_science',
                                            'high_school_mathematics',
                                            'high_school_physics',
                                            'high_school_statistics',
                                            'machine_learning'],
                            'split': 'test'},
    'mmlu_social_sciences_generative': {'dataset_path': 'hails/mmlu_no_train', 
                            'dataset_name': ['econometrics',
                                             "high_school_geography",
                                            'high_school_government_and_politics',
                                            'high_school_macroeconomics',
                                            'high_school_microeconomics',
                                            'high_school_psychology',
                                            'human_sexuality',
                                            'professional_psychology',
                                            'public_relations',
                                            'security_studies',
                                            'sociology',
                                            'us_foreign_policy'],
                            'split': 'test'},
    'mmlu_humanities_generative': {'dataset_path': 'hails/mmlu_no_train', 
                                'dataset_name': ['formal_logic',
                                                'high_school_european_history',
                                                'high_school_us_history',
                                                'high_school_world_history',
                                                'international_law',
                                                'jurisprudence',
                                                'logical_fallacies',
                                                'moral_disputes',
                                                'moral_scenarios',
                                                'philosophy',
                                                'prehistory',
                                                'professional_law',
                                                'world_religions',
                                                ], 
                                'split': 'test'},
    'mmlu_other_generative': {'dataset_path': 'hails/mmlu_no_train', 
                            'dataset_name': ['business_ethics',
                                            'clinical_knowledge',
                                            'college_medicine',
                                            'global_facts',
                                            'human_aging',
                                            'management',
                                            'marketing',
                                            'medical_genetics',
                                            'miscellaneous',
                                            'nutrition',
                                            'professional_accounting',
                                            'professional_medicine',
                                            'virology',
                                            ], 
                            'split': 'test'},
    'bbh_zeroshot': {'dataset_path' : 'SaylorTwift/bbh', 'dataset_name': None, 'split': 'test'},
    'bbh_zeroshot_subset': {'dataset_path' : 'SaylorTwift/bbh', 
                            'dataset_name': 
                            [
                                'boolean_expressions',
                                'causal_judgement',
                                'formal_fallacies',
                                'logical_deduction_five_objects',
                                'navigate',
                                'object_counting'
                            ],
                            'split': 'test'},
    'bbh_fewshot':{'dataset_path' : 'SaylorTwift/bbh', 'dataset_name': None, 'split': 'test'},
    'bbh_fewshot_subset': {'dataset_path' : 'SaylorTwift/bbh', 
                            'dataset_name': 
                            [
                                'boolean_expressions',
                                'causal_judgement',
                                'formal_fallacies',
                                'logical_deduction_five_objects',
                                'navigate',
                                'object_counting'
                            ],
                            'split': 'test'},
    'swde': {'dataset_path' : 'hazyresearch/based-swde', 'dataset_name': None, 'split': 'validation'},
    'humaneval': {'dataset_path' : 'openai/openai_humaneval', 'dataset_name': None, 'split': 'test'},
    'humaneval_instruct' : {'dataset_path' : 'openai/openai_humaneval', 'dataset_name': None, 'split': 'test'},
    'winogrande_gen' : {'dataset_path' : 'allenai/winogrande', 'dataset_name' :'winogrande_xl', 'split': 'test'},

}