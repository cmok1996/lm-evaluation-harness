TASK_CONFIG = {
    "ifeval": {
        "dataset_path": "google/IFEval",
        "dataset_name": None,
        "split": "train",
        "benchmark_category": "alignment",
        "usage": "short_prompt_medium_output",
        "n_shot": 0,
        "metrics": "prompt_level_strict_acc,none",
        "n_samples": 541,
    },
    "gsm8k": {
        "dataset_path": "openai/gsm8k",
        "dataset_name": "main",
        "split": "test",
        "benchmark_category": "multistep_reasoning",
        "usage": "medium_prompt_short_output",
        "n_shot": 5,
        "metrics": "exact_match,flexible-extract",
        "n_samples": 1319,
    },
    "hellaswag_gen": {
        "dataset_path": "Rowan/hellaswag",
        "dataset_name": None,
        "split": "test",
        "benchmark_category": "commonsense",
        "usage": "short_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,get_response",
        "n_samples": 10042,
    },
    "drop_new": {
        "dataset_path": "EleutherAI/drop",
        "dataset_name": None,
        "split": "validation",
        "benchmark_category": "comprehension",
        "usage": "medium_prompt_short_output",
        "n_shot": 3,
        "metrics": "f1,none",
        "n_samples": 9536,
    },
    "truthfulqa_gen": {
        "dataset_path": "truthfulqa/truthful_qa",
        "dataset_name": "generation",
        "split": "validation",
        "benchmark_category": "alignment",
        "usage": "short_prompt_short_output",
        "n_shot": 0,
        "metrics": "bleu_acc,none",
        "n_samples": 817,
    },
    "mmlu_generative": {
        "dataset_path": "hails/mmlu_no_train",
        "dataset_name": None,
        "split": "test",
        "benchmark_category": "general_knowledge",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,get_response",
        "n_samples": 14042,
    },
    "mmlu_generative_orig": {
        "dataset_path": "hails/mmlu_no_train",
        "dataset_name": None,
        "split": "test",
        "benchmark_category": "general_knowledge",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,get_response",
        "n_samples": 14042,
    },
    "mmlu_stem_gen": {
        "dataset_path": "hails/mmlu_no_train",
        "dataset_name": [
            "abstract_algebra",
            "anatomy",
            "astronomy",
            "college_biology",
            "college_chemistry",
            "college_computer_science",
            "college_mathematics",
            "college_physics",
            "computer_security",
            "conceptual_physics",
            "electrical_engineering",
            "elementary_mathematics",
            "high_school_biology",
            "high_school_chemistry",
            "high_school_computer_science",
            "high_school_mathematics",
            "high_school_physics",
            "high_school_statistics",
            "machine_learning",
        ],
        "split": "test",
        "benchmark_category": "general_knowledge",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,get_response",
        "n_samples": 3153,
    },
    "mmlu_stem_gen_orig": {
        "dataset_path": "hails/mmlu_no_train",
        "dataset_name": [
            "abstract_algebra",
            "anatomy",
            "astronomy",
            "college_biology",
            "college_chemistry",
            "college_computer_science",
            "college_mathematics",
            "college_physics",
            "computer_security",
            "conceptual_physics",
            "electrical_engineering",
            "elementary_mathematics",
            "high_school_biology",
            "high_school_chemistry",
            "high_school_computer_science",
            "high_school_mathematics",
            "high_school_physics",
            "high_school_statistics",
            "machine_learning",
        ],
        "split": "test",
        "benchmark_category": "general_knowledge",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,get_response",
        "n_samples": 3153,
    },
    "mmlu_social_sciences_gen": {
        "dataset_path": "hails/mmlu_no_train",
        "dataset_name": [
            "econometrics",
            "high_school_geography",
            "high_school_government_and_politics",
            "high_school_macroeconomics",
            "high_school_microeconomics",
            "high_school_psychology",
            "human_sexuality",
            "professional_psychology",
            "public_relations",
            "security_studies",
            "sociology",
            "us_foreign_policy",
        ],
        "split": "test",
        "benchmark_category": "general_knowledge",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,get_response",
        "n_samples": 3077,
    },
    "mmlu_social_sciences_gen_orig": {
        "dataset_path": "hails/mmlu_no_train",
        "dataset_name": [
            "econometrics",
            "high_school_geography",
            "high_school_government_and_politics",
            "high_school_macroeconomics",
            "high_school_microeconomics",
            "high_school_psychology",
            "human_sexuality",
            "professional_psychology",
            "public_relations",
            "security_studies",
            "sociology",
            "us_foreign_policy",
        ],
        "split": "test",
        "benchmark_category": "general_knowledge",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,get_response",
        "n_samples": 3077,
    },
    "mmlu_humanities_gen": {
        "dataset_path": "hails/mmlu_no_train",
        "dataset_name": [
            "formal_logic",
            "high_school_european_history",
            "high_school_us_history",
            "high_school_world_history",
            "international_law",
            "jurisprudence",
            "logical_fallacies",
            "moral_disputes",
            "moral_scenarios",
            "philosophy",
            "prehistory",
            "professional_law",
            "world_religions",
        ],
        "split": "test",
        "benchmark_category": "general_knowledge",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,get_response",
        "n_samples": 4705,
    },
    "mmlu_humanities_gen_orig": {
        "dataset_path": "hails/mmlu_no_train",
        "dataset_name": [
            "formal_logic",
            "high_school_european_history",
            "high_school_us_history",
            "high_school_world_history",
            "international_law",
            "jurisprudence",
            "logical_fallacies",
            "moral_disputes",
            "moral_scenarios",
            "philosophy",
            "prehistory",
            "professional_law",
            "world_religions",
        ],
        "split": "test",
        "benchmark_category": "general_knowledge",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,get_response",
        "n_samples": 4705,
    },
    "mmlu_other_gen": {
        "dataset_path": "hails/mmlu_no_train",
        "dataset_name": [
            "business_ethics",
            "clinical_knowledge",
            "college_medicine",
            "global_facts",
            "human_aging",
            "management",
            "marketing",
            "medical_genetics",
            "miscellaneous",
            "nutrition",
            "professional_accounting",
            "professional_medicine",
            "virology",
        ],
        "split": "test",
        "benchmark_category": "general_knowledge",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,get_response",
        "n_samples": 3107,
    },
    "mmlu_other_gen_orig": {
        "dataset_path": "hails/mmlu_no_train",
        "dataset_name": [
            "business_ethics",
            "clinical_knowledge",
            "college_medicine",
            "global_facts",
            "human_aging",
            "management",
            "marketing",
            "medical_genetics",
            "miscellaneous",
            "nutrition",
            "professional_accounting",
            "professional_medicine",
            "virology",
        ],
        "split": "test",
        "benchmark_category": "general_knowledge",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,get_response",
        "n_samples": 3107,
    },
    "bbh_zeroshot": {
        "dataset_path": "SaylorTwift/bbh",
        "dataset_name": None,
        "split": "test",
        "benchmark_category": "multistep_reasoning",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,flexible-extract",
        "n_samples": 6511,
    },
    "bbh_zeroshot_subset": {
        "dataset_path": "SaylorTwift/bbh",
        "dataset_name": [
            "boolean_expressions",
            "causal_judgement",
            "formal_fallacies",
            "logical_deduction_five_objects",
            "navigate",
            "object_counting",
        ],
        "split": "test",
        "benchmark_category": "multistep_reasoning",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,flexible-extract",
        "n_samples": 1437,
    },
    "bbh_fewshot": {
        "dataset_path": "SaylorTwift/bbh",
        "dataset_name": None,
        "split": "test",
        "benchmark_category": "multistep_reasoning",
        "usage": "medium_prompt_short_output",
        "n_shot": 3,
        "metrics": "exact_match,get_response",
        "n_samples": 6511,
    },
    "bbh_fewshot_orig": {
        "dataset_path": "SaylorTwift/bbh",
        "dataset_name": None,
        "split": "test",
        "benchmark_category": "multistep_reasoning",
        "usage": "medium_prompt_short_output",
        "n_shot": 3,
        "metrics": "exact_match,none",
        "n_samples": 6511,
    },
    "bbh_fewshot_subset": {
        "dataset_path": "SaylorTwift/bbh",
        "dataset_name": [
            "boolean_expressions",
            "causal_judgement",
            "formal_fallacies",
            "logical_deduction_five_objects",
            "navigate",
            "object_counting",
        ],
        "split": "test",
        "benchmark_category": "multistep_reasoning",
        "usage": "medium_prompt_short_output",
        "n_shot": 3,
        "metrics": "exact_match,get_response",
        "n_samples": 1437,
    },
    "bbh_fewshot_subset_orig": {
        "dataset_path": "SaylorTwift/bbh",
        "dataset_name": [
            "boolean_expressions",
            "causal_judgement",
            "formal_fallacies",
            "logical_deduction_five_objects",
            "navigate",
            "object_counting",
        ],
        "split": "test",
        "benchmark_category": "multistep_reasoning",
        "usage": "medium_prompt_short_output",
        "n_shot": 3,
        "metrics": "exact_match,none",
        "n_samples": 1437,
    },
    "swde": {
        "dataset_path": "hazyresearch/based-swde",
        "dataset_name": None,
        "split": "validation",
        "benchmark_category": "comprehension",
        "usage": "long_prompt_short_output",
        "n_shot": 0,
        "metrics": "contains,none",
        "n_samples": 1111,
    },
    "swde_orig": {
        "dataset_path": "hazyresearch/based-swde",
        "dataset_name": None,
        "split": "validation",
        "benchmark_category": "comprehension",
        "usage": "long_prompt_short_output",
        "n_shot": 0,
        "metrics": "contains,none",
        "n_samples": 1111,
    },
    "humaneval": {
        "dataset_path": "openai/openai_humaneval",
        "dataset_name": None,
        "split": "test",
        "benchmark_category": "coding",
        "usage": "medium_prompt_medium_output",
        "n_shot": 0,
        "metrics": "pass@1,create_test_delimiter",
        "n_samples": 164,
    },
    "humaneval_orig": {
        "dataset_path": "openai/openai_humaneval",
        "dataset_name": None,
        "split": "test",
        "benchmark_category": "coding",
        "usage": "medium_prompt_medium_output",
        "n_shot": 0,
        "metrics": "pass@1,create_test",
        "n_samples": 164,
    },
    "winogrande_gen": {
        "dataset_path": "allenai/winogrande",
        "dataset_name": "winogrande_xl",
        "split": "test",
        "benchmark_category": "commonsense",
        "usage": "short_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,get_response",
        "n_samples": 1267,
    },
    "wmt24": {
        "dataset_path": "google/wmt24pp",
        "dataset_name": None,
        "split": "train",
        "benchmark_category": "multilingual",
        "usage": "medium_prompt_medium_output",
        "n_shot": 0,
        "metrics": "bleu,none",
        "n_samples": 54890,
    },
    "wmt24_subset": {
        "dataset_path": "google/wmt24pp",
        "dataset_name": ["en-ar_EG", "en-zh_CN", "en-de_DE", "en-es_MX", "en-fr_CA", "en-it_IT", "en-ja_JP", "en-ta_IN", "en-th_TH", "en-vi_VN"],
        "split": "train",
        "benchmark_category": "multilingual",
        "usage": "medium_prompt_medium_output",
        "n_shot": 0,
        "metrics": "bleu,none",
        "n_samples": 9980,
    },
    "wmt24_subset": {
        "dataset_path": "google/wmt24pp",
        "dataset_name": ["en-ar_EG", "en-zh_CN", "en-de_DE", "en-es_MX", "en-fr_CA", "en-it_IT", "en-ja_JP", "en-ta_IN", "en-th_TH", "en-vi_VN"],
        "split": "train",
        "benchmark_category": "multilingual",
        "usage": "medium_prompt_medium_output",
        "n_shot": 0,
        "metrics": "bleu,none",
        "n_samples": 9980,
    },
    "mgsm_direct_zh": {
        "dataset_path": "juletxara/mgsm",
        "dataset_name": ["zh"],
        "split": "test",
        "benchmark_category": "multilingual",
        "usage": "medium_prompt_short_output",
        "n_shot": 0,
        "metrics": "exact_match,flexible-extract",
        "n_samples": 250,
    },
}

PREFERRED_BENCHMARKS = {
    "general_knowledge": "mmlu_generative",
    "commonsense": "wingogrande_gen",
    "comprehension": "drop_new",
    "multistep_reasoning": "bbh_fewshot_subset",
    "alignment": "ifeval",
    "coding": "humaneval",
    "multilingual": "mgsm_direct_zh"
}